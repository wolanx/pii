# 算法工程师-高级深度学习

- pytorch doc https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html
- pytorch基础入门教程/一小时学会pytorch https://blog.csdn.net/weixin_41070748/article/details/89890330
- 《动手学深度学习》 https://zh.d2l.ai/index.html

## 第二章 神经网络深⼊

- 第1节 从优化问题讲起 I
- 第2节 【实战】拟合问题
    - 代码 [demo2-2_line/demo2-2.ipynb](demo2-2_line/demo2-2.ipynb)
    - model = nn.Linear(in_features=1, out_features=1) # 单参数
    - loss_fn = nn.MSELoss()
    - optimizer = torch.optim.SGD(model.parameters(), lr=.01)
- 第3节 从优化问题讲起 II
    - gradient descent
    - momentum 动量：相当于 lr 加个 a加速度。为了解决 N 波峰，无法越过峰问题
    - AdaGrad 没太懂，但是变慢很多，lr / 区域面积 导致？梯度与lr反比，大的反而小
    - RMSProp 针对AdaGrad太慢，添加一个（区域面积）滑动窗口
    - Adam 更进一步
- 第4节 【实战】优化方法比较
    - 可视化工具 https://github.com/lilipads/gradient_descent_viz
- 第5节 深度神经网络
    - 激活 Function
        - sigmoid `范围：0 ~ 1`
        - tanh(x) = 2 * sigmoid(2 * x) - 1 `范围：-1 ~ 1`
        - ReLU `范围：>= 0`
        - leaky relu / p-relu 针对 左半区（死区）优化
    - 深度 = 学习力 广度 = 记忆力
- 第6节 【实战】使用神经网络建模MNIST数据
    - 代码 [demo2-6_mnist/demo2-6.ipynb](demo2-6_mnist/demo2-6.ipynb)
- 第7节 【实战】激活函数与优化方法
    - F.relu torch.optim 切换，可视化展示
- 第8节 正则化方法 I
    - training set - validation set - test set
- 第9节 正则化方法 II
    - 成本函数 + 正则项
        - L1-Norm(Lasso) 稀疏参数`空间` (少量非零参数)
        - L2-Norm(Ridge) 限制参数`范围` (参数接近于零)
    - early stopping 提前结束，避免 over-fit
    - convolution 卷积 参数共享 input 组合后给 output
    - dropout 系综方法 随机 取消一批
- 第10节 【实战】正则化方法
    - L2 的运用
- 第11节 模型性能评价
    - 二分类
        - TP True Positive 真阳性
        - TN True Negative 真阴性
        - FP False Positive 假阳性
        - FN False Negative 假阴性
    - 多分类混淆矩阵 （色阶图：比例高的深）
    - ROC 曲线
- 第12节 【实战】模型性能评价指标
    - 只给 accuracy 显得很业余，F1 score 能更立体
        - sensitivity = TP / (TP + FN + .0) 敏感性 召回率(recall)
        - specificity = TN / (TN + FP + .0) 特异度
        - precision = TP / (TP + FP + .0) 精确性
        - accuracy = (TP + TN) / (TP + FN + FP + TN + .0) 准确性
        - f1 = 2 * sensitivity * precision / (sensitivity + precision)
    - 分领域 关注点不同
        - 反欺诈 recall + precision => f1
        - 医学 specificity
        - 新手 accuracy
- 第13节 深度学习能力边界
    - 逻辑推理问题
    - 少数据量问题
    - 难度大于一秒
- 第14节 练习作业
    - mnist

## 第三章 图像分类与目标检测

- 第1节 卷积的基本概念
    - 池化 * [[0, 1], [-1 ,0]] 把 img 转成 浮雕
    - stride 步伐大小
    - MaxPool框内最大 AvgPool框内平均
    - 5*5 stride=2 会padding=1在（右下）边 6*6 3*3
- 第2节 【实战】异构深度学习环境搭建
    - ubuntu
    - $ nvidia-smi # 看功率，内存
- 第3节 【实战】卷积层的实现
    - keras 简化 tf
    - Conv2d 深度理解
        - 通道（Channel），特征图（feature map）
        - channel 个 过滤器
        - 回到mnist，前两步conv2d做特征提取，后全连接，加随机剔除（dropout）特征
        - 卷积网络中的通道(Channel)理解 https://blog.csdn.net/Limonor/article/details/106979250
- 第4节 典型卷积神经网络
    - 28*28*1 => 14*14*16 => 7*7*32 => 256 => 10
    - 形状介绍 Model Zoo
        - VGG Conv64-128-256-512-512 => FC-6144-6144-1000 => Softmax
        - GoogleNet
        - ResNet 高速通道，冗余结构(前一层的参数快速融入下一阶段)
- 第5节 【实战】简单的卷积神经网
    - keras layer 实现 cnn
- 第6节 模型结构与实战
    - LeNet
        - 参数的计算 shape => param conv: 5*5*6+6, fc: 400*120+120
        - model.summary() keras直观看
        - fc 参数占比 +90% conv反而小
    - AlexNet
        - 2012年
        - 第一层conv padding用了valid
    - VGG
        - 2014年
        - 回归本质 kernel_size=(3, 3) stride=(1, 1) padding=same relu
        - vgg19 比 vgg16 在第345轮各多一次conv
    - ResNet
- 第7节 【实战】ResNet
    - 代码 [demo3-7_use_cnn](demo3-7_use_cnn)
    - residual: 残差 identify 把第一层的x，加上第二层f(x)，结合到第三层 g(x) = f(x) + x
        - 解决layer多，反而err大的问题
        - ResNet详解+PyTorch实现 https://blog.csdn.net/frighting_ing/article/details/121324000
    - BN(batch normalization)（批归一化）值域拉回 (-1 1)
        - conv => bn => relu 中间
        - 标准归一化 z~ = z - μ / σ (moving avg: 移动平均值，滑动窗口) 独立同分布
        - 训练归一化 z^ = γ * z~ + β
    - bottleneck 瓶颈
        - 好处：减低参数数量
        - 第2345轮各多一次conv
        - conv3x3 relu conv3x3 (identify) (relu)
        - conv1x1 relu conv3x3 relu conv1x1 (identify) (relu)
        - 比原本的 identify 转维方便
    - 常用特征提取器
        - ResNet-18 block (2 2 2 2) (62 128 256 512)
        - ResNet-34 block (3 4 6 3) (62 128 256 512)
            - 计算 34 = 1（卷积） + 1（池化） + 3 * 2 + 4 * 2 + 6 * 2 + 3 * 2
        - ResNet-50 block (3 4 6 3) (256 512 1024 2048)
            - 计算 50 = 1（卷积） + 1（池化） + 3 * 3 + 4 * 3 + 6 * 3 + 3 * 3
                - 50 vs 34 block 相同，但是 bottleneck 多(3+4+6+3)个conv1
        - ResNet-101 block (3 4 23 3) (256 512 1024 2048)
        - ResNet-152 block (3 8 36 3) (256 512 1024 2048)
- 第8节 目标检测
    - R-CNN (Regions with CNN features)
        - backbone cnn的out虽然是classify，但前一层是features，包含更多的信息
    - selective search (box rect)
        - IoU intersection-over-union 交集并集
    - AlexNet 同2013年
    - mAP (mean average precision) P/R曲线（准确率/召回率：面积越大越好=mAP）
    - svm 支持向量机
        - 核函数（kernel）
    - R-CNN 缺陷
        - multi-stage 3阶段分开
        - expensive in space and time
        - obj detection 慢
    - Fast R-CNN 9x time, 213x test-time, mAP 更高
        - Rol Pooling （解决维度不同的另一个方式）剩余边不处理
    - Faster R-CNN
        - 对 selective search 当 cnn 处理
    - YOLO (You Only Look Once), Fast YOLO
- 第9节 【实战】Faster R-CNN
    - Fast vs Faster
- 第10节 【实战】表征学习
    - 代码 [demo3-10_tsne/demo3-10.ipynb](demo3-10_tsne/demo3-10.ipynb)
- 第11节 第二章习题讲解
    - mnist
    - 可视化 img, label, result
    - 画混淆矩阵
- 第12节 练习作业
    - mask R-CNN

## 第四章 图像分割

- 第1节 图像分割基础
    - 原始图像 => 像素图像的预测
    - 语义分割（Semantic segmentation） => 实例分割（Instance segmentation）
    - 全卷积神经网络
        - Encoder 大 => 小 conv (Max Pooling)
        - Decoder 小 => 大 deconv (Up Sampling)
    - Transpose Conv (转置卷积), Deconv (反卷积)
    - dilated(扩张;近义词：atrous) 空洞卷积
        - rate=2 rate=4 rate=8 rate=16 rate大=视野越大
- 第2节 【实战】Deconvolution与空洞卷积
    - 转置卷积 nn.ConvTranspose2d
    - 空洞卷积 nn.Conv2d(dilation=2)
        - 普通卷积 nn.Conv2d(dilation=1)
- 第3节 图像分割模型
    - U-Net 对小分辨率的feature不丢失
    - SPP (Spatial Pyramid Pooling)(空间金字塔池)
    - FPN (Feature Pyramid Networks)
    - DeepLab-v3 （CRF 弃用）
    - mIOU 丢失性的变化，和 mAP 同级
    - DeepLab-v3+
        - SPP + Encode-Decode
- 第4节 【实战】U-Net
    - 代码 [demo4-4_unet/demo4-4_unet.ipynb](demo4-4_unet/demo4-4_unet.ipynb)
    - 语义分割网络U-Net详解 https://zhuanlan.zhihu.com/p/389949794
        - conv1 pool1, conv2 pool2, conv3 pool3, conv4 pool4, conv5
        - up6 = Con(conv4 + deconv(conv5)) conv6 (Con: Concatenate)
        - up7 = Con(conv3 + deconv(conv6)) conv7
        - up8 = Con(conv2 + deconv(conv7)) conv8
        - up9 = Con(conv1 + deconv(conv8)) conv8
- 第5节 【实战】DeepLab v3
    - 代码 [demo4-5_deeplab_v3plus/demo4-5_deeplab_v3plus.ipynb](demo4-5_deeplab_v3plus/demo4-5_deeplab_v3plus.ipynb)
    - aspp dilation (1, 2, 3, 4) * 6
- 第6节 模型可视化
    - 白盒
        - 卷积核（filter）
        - 特征图（feature map）
        - 特征向量（feature vector）（表征：tsne）
    - 黑盒
        - 遮盖（occlusion）P的热力图
        - 显著梯度（saliency map）
- 第7节 【实战】特征图像可视化
    - 代码 [demo4-7_nn_plt/demo4-7_nn_plt.ipynb](demo4-7_nn_plt/demo4-7_nn_plt.ipynb)
        - torch.nn.Module.register_forward_hook 获取到特征图
        - torchvision.utils.make_grid 特征图拼接成一个网格状排列的大图
        - torchvision.utils.save_image 保存图片
- 第8节 病理影像分割初探 ★
    - 模型研发流水线
        - 收集数据
        - 数据清洗
        - 领域知识
        - 模型训练
        - 模型调优
        - 模型上线
    - 数据预处理
        - 二值化的过滤器 => 有效区域
        - 有效区域 => 小块
        - 生成训练图像对（图像块，标注）
        - 数据增强（翻转&旋转，随机放缩，颜色扰动）
- 第9节 自监督学习 ★
    - 照片上色
    - 上下文预测（给两张图，求第二张图在第一张图的什么位置）
    - 拼图游戏（九宫格还原）
    - 图像补全，对抗网络（GAN）
    - classify corrupted images（已损坏）
    - distortion（扭曲）4旋转4分类。重心区域识别
    - SimCLR（MoCo, MoCo2）把各种变形后的和原图，相关性绑定
    - Transformers，自然语言处理，在cv中的运用
        - 开荒之作：Vision Transformer (ViT)
- 第10节 模型训练流程
    - 成本函数
        - classification
            - softmax_cross_entropy_with_logits_v2
            - sparse_softmax_cross_entropy_with_logits
            - weighted_cross_entropy_with_logits
        - regression
            - mse mean_squared_error
    - lr的调整
        - 阶梯下降
    - 模型保存加载
- 第11节 第三章习题讲解
    - ResNet 34vs50 参数区别不大的原因
- 第12节 练习作业
    - DeepLab v3+
    - 3.8节中的各类图像增强方法
    - Context Prediction的代码实现
    - MoCo与MoCo v2

## 第五章 ⾼级循环神经网络

- 第1节 自然语言处理基础
    - NLP
    - 编码
        - one hot 矩阵存词词对应
        - bow (bag of words) 单词计数
        - word embedding 词嵌入
    - word2vec
- 第2节 循环神经网络
    - one to one
    - one to many 一个词联想出一段故事
    - many to one 推测下一段话
    - many to many
    - model
        - RNN = Vanilla Recurrent Neural Network (U, V -> W 类似 fc)
        - LSTM = Long Short-Term Memory (上一时刻) Ct Ht 的复杂引出 GRU
        - GRU = Gated Recurrent Unit (但准确度还是LSTM高)
- 第3节 【实战】RNN及其变种
    - 代码 [demo5-3_rnn_mnist/demo5-3_rnn_mnist.ipynb](demo5-3_rnn_mnist/demo5-3_rnn_mnist.ipynb)
- 第4节 基于会话的欺诈检测
    - account hijacking 账户侵入
    - card faking
    - imbalanced data
- 第5节 【实战】欺诈检测的RNN实现
    - 代码 todo
- 第6节 注意力模型 ★
    - transformer 以前 最为流行
    - self attention
        - position 位置记录，支持时序
        - multi-head
- 第7节 第四章习题讲解
    - aspp: add => concatenate
- 第8节 直击面试I ★
    - 20 题！！！
- 第9节 练习作业
    - 多层GRU
    - 基于会话的欺诈检测训练代码中的sequence length
    - 抓取文本实现Word2Vec模型训练（基于gensim库）

## 第六章 分布式深度学习系统

- 第1节 分布式系统
    - hadoop
    - spark
    - acid 理论 atomicity consistency isolation durability
    - cap 原则 整体集群 consistency availability partition-tolerance
        - ca RDBMS
        - cp mongodb hbase redis
        - ap couchdb cassandra dynamodb riak
    - base 理论 切实可行
        - 基本可行 basically available
        - 软状态 soft state
        - 最终一致性 eventually consistent
- 第2节 分布式深度学习系统
    - i7-7700k ~640 GFLOPs ~0.64 TFLOPs
    - GTX3090 ~35.6 TFLOPs 模型平均快60倍
    - BSP 同步通信 Bulk Synchronous Parallel
    - SSP 同步与异步通信结合 Stale Synchronous Parallel
- 第3节 【实战】数据并行模型训练
    - cpu + N * gpu
- 第4节 微服务架构
    - 单服务易升级
- 第5节 【实战】使用Kafka搭建MQ
    - `kafka-console-producer --broker-list   localhost:32773 --topic chat`
    - `kafka-console-consumer --broker-server localhost:32773 --topic chat --form-beginning`
- 第6节 分布式推理系统
    - inference 推理系统
    - serving 支持多模型推理
- 第7节 【实战】TensorFlow Serving in Docker
    - nvidia-docker run -it -v .:/models/model -p8500:8500 tensorflow/serving:nightly-gpu
- 第8节 第五章习题讲解
    - 通过masking辅助计算length
- 第9节 直击面试II ★
    - 20 题！！！

## 第七章 深度学习前沿

- 第1节 深度增强学习
    - 控制机器人的行为
    - reinforcement learning (RL)
        - π Policy 策略
        - a = π(s) 某个状态下的行为
        - Qπ(s,a) Value function 收益函数
    - approaches to RL
        - Policy-based RL 基于策略的
        - Value-based RL 求收益最大值。使用最多
        - Model-based RL 需要对环境，对环境要求高
    - Bellman Equation 贝尔曼方程
        - Qπ(s,a) ≈ Q(s,a,w)
        - error = ∑( (r + y maxQ(s',a',w)) - Q(s,a,w) )²
    - Deep Q-Networks(DQN)
        - Use experience replay 把状态对和当下的收益存下来，可以把时间依赖取消掉
        - Freeze target
        - Clip rewards 限制奖励值
    - 论文：谷歌2015，Example: Breakout
- 第2节 【实战】Flappy Bird
- 第3节 AlphaGo
- 第4节 生成对抗网络
- 第5节 【实战】SimpleGAN
- 第6节 【实战】ConditionalGAN
- 第7节 【实战】CycleGAN
- 第8节 未来在哪里
- 第9节 练习作业
- 第10节 第六章习题讲解
- 第11节 直击面试III

## 第八章 专题讲座

- 【Lecture 1】DenseNet
- 【Lecture 2】Inception
- 【Lecture 3】Xception
- 【Lecture 4】ResNeXt
- 【Lecture 5】Transformer和它的朋友们
- 【Lecture 6】深度学习产品化
- 【Lecture 7】果壳中的量子计算
- 【Lecture 8】人工智能产业

## 第九章 应⽤于大规模数据集的图像分类模型

- 第1节 核心实战概述
- 第2节 ImageNet介绍
- 第3节 数据探索与预处理
- 第4节 数据队列
- 第5节 通用数据队列
- 第6节 建立模型结构
- 第7节 MNIST数据集训练
- 第8节 ImageNet Tiny数据集训练
- 第9节 猫狗大战数据集介绍与预处理
- 第10节 模型测试代码
- 第11节 模型训练与过程分析
- 第12节 模型批量测试与性能指标
- 第13节 ResNet家族模型的表现
- 第14节 常见模型的表现
- 第15节 作业合集

## 第十章 建立病理影像的病变区域分割模型

- 第1节 数字病理切片介绍
- 第2节 数字病理切片预处理
- 第3节 样本均衡性处理
- 第4节 经典数据队列
- 第5节 建立训练模型
- 第6节 实现测试逻辑
- 第7节 预测结果后处理
- 第8节 20x模型训练与测试
- 第9节 40x模型训练与测试
- 第10节 DeepLabv3Plus模型训练与测试
- 第11节 论文串烧：BMJ Open
- 第12节 论文串烧：Clinical and Translational Medicine
- 第13节 论文串烧：Nature Communications
- 第14节 论文串烧：ICCV
- 第15节 论文串烧：ECML
- 第16节 论文串烧：Nature Medicine
- 第17节 第九章习题讲解

## 第十一章 分布式深度学习推理系统

- 第1节 系统架构设计
- 第2节 代码结构与调度器配置
- 第3节 调度器核心逻辑
- 第4节 自定义Logging机制
- 第5节 工作节点基础代码
- 第6节 工作节点任务处理
- 第7节 工作节点核心逻辑
- 第8节 日志模块编写
- 第9节 代码调试环境搭建
- 第10节 运行Celery任务
- 第11节 模型导出与运行
- 第12节 系统整体运行
- 第13节 构建Docker镜像的原始方法
- 第14节 使用Dockerfile构建镜像
- 第15节 增加病理影像预测Task
- 第16节 分布式系统研究结果
- 第17节 TensorFlow Serving的内部机制

## 第十二章 课程总结

- 课程总结I：深度学习理论
- 课程总结II：卷积神经网络
- 课程总结III：循环神经网络
- 课程总结IV：深度学习系统与前沿
