{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST\n",
    "- 官方实现 https://github.com/pytorch/examples/blob/main/mnist/main.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    # transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # 归一化到(0,1) 分布到(-1,1)\n",
    "])\n",
    "dataset1 = torchvision.datasets.MNIST(root=\"../../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = torchvision.datasets.MNIST(root=\"../../data\", train=False, download=False, transform=transform)\n",
    "# DataLoader num_workers 不要设置, cpu: num_workers=2\n",
    "train_loader: DataLoader = torch.utils.data.DataLoader(dataset1, batch_size=64, shuffle=True)\n",
    "test_loader: DataLoader = torch.utils.data.DataLoader(dataset2, batch_size=1000, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    plt.imshow(np.reshape(img, [28, 28]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "img, val = train_loader.dataset[0]\n",
    "print(f\"target: {val}\")\n",
    "imshow(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def train(model: nn.Module, device, train_loader, optimizer: torch.optim.Optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            # Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.142410\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "\n",
    "\n",
    "def test(model: nn.Module, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdims=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # Test set: Average loss: 0.0483, Accuracy: 9849/10000 (98%)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.Conv2d 卷积\n",
    "- 图示 https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "- in_channels: 输入的通道数目\n",
    "- out_channels: 输出的通道数目\n",
    "- kernel_size: w * h\n",
    "- stride: 卷积每次滑动的步长为多少，默认1\n",
    "- padding: 加 x 外圈边界\n",
    "- dilation: kernel 每次的 偏移量，默认1，2就是跳一个\n",
    "\n",
    "### nn.Dropout\n",
    "- x % 的概率失效\n",
    "\n",
    "### size 的 计算\n",
    "- Conv2d\n",
    "    - nn.Conv2d(1, 32, (3, 3), (1, 1))\n",
    "    - 1, 28, 28 => 32, 26, 26\n",
    "    - nn.Conv2d(32, 64, (3, 3), (1, 1))\n",
    "    - 32, 26, 26 => 64, 24, 24\n",
    "- max_pool2d 大步走 `2步变1步` size 小一半\n",
    "    - F.max_pool2d(x, 2)\n",
    "    - 64, 24, 24 => 64, 12, 12\n",
    "- flatten 拍平\n",
    "    - torch.flatten(x, 1)\n",
    "    - 64, 12, 12 => 9216"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class MyModel(nn.Module): # e1 99%\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, 5), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=(1, 1))\n",
    "        self.dropout1 = nn.Dropout(.25)\n",
    "        self.dropout2 = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):  # batch_size=64, 1, 28, 28\n",
    "        x = self.conv1(x)  # 32, 26, 26\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)  # 64, 24, 24\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.max_pool2d(x, 2)  # 64, 12, 12\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)  # 9216\n",
    "        x = self.fc1(x)  # 64, 128\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)  # 10\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)  # 10\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyModel2(nn.Module):  # e1 86% e5 92%\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):  # 64, 1, 28, 28 第一个 64 是 batch_size\n",
    "        x = torch.flatten(x, 1)  # 728\n",
    "        x = self.fc1(x)  # 200\n",
    "        x = self.fc2(x)  # 200\n",
    "        x = self.fc3(x)  # 10\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)  # 10\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 200]         157,000\n",
      "            Linear-2                  [-1, 200]          40,200\n",
      "            Linear-3                   [-1, 10]           2,010\n",
      "================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.76\n",
      "Estimated Total Size (MB): 0.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MyModel2().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1)\n",
    "\n",
    "scheduler = StepLR(optimizer=optimizer, step_size=1, gamma=.7)\n",
    "\n",
    "torchsummary.summary(model, (1, 28, 28))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.292221\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.373076\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.653551\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.711813\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.457500\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.549031\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.761111\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.478221\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.513730\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.412486\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.494328\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.321828\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.379031\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.442548\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.630965\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.441282\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.456119\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.122255\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.489996\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.778697\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.747546\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.507483\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.348413\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.325670\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.497407\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.296740\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.457689\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.464621\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.443267\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.337275\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.401120\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.220679\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.337379\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.384611\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.405751\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.721341\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.396978\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.489698\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.273643\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.449686\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.255544\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.292130\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.315659\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.399943\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.368353\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.324060\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.238620\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.421949\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.280528\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.577202\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.677301\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.488871\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.284603\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.477835\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.427190\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.270573\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.595468\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.196108\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.222832\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.415943\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.401184\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.287965\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.278290\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.491892\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.304213\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.390294\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.437074\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.361130\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.456135\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.348468\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.417438\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.448546\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.366188\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.370156\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.444808\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.263186\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.644042\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.299780\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.330224\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.310358\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.656408\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.514747\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.344006\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.483070\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.307307\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.310044\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.415493\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.584959\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.227688\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.340884\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.300934\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.441959\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.338478\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.151383\n",
      "\n",
      "Test set: Average loss: 0.3785, Accuracy: 8881/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.271888\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.229980\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.386530\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.399902\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.258366\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.496214\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.725529\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.297392\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.312099\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.267583\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.401907\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.394558\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.251918\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.310604\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.205237\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.576684\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.213120\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.435804\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.261617\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.135003\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.530365\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.378445\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.450435\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.208584\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.277646\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.579611\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.565058\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.188046\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.683629\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.267908\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.417624\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.177185\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.292408\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.305639\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.300892\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.246025\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.290108\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.556597\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.335707\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.580831\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.339345\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.377350\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.260230\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.122022\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.287425\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.500230\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.178251\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.255540\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.220699\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.448135\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.647318\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.239214\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.584476\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.274759\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.611780\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.220511\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.468654\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.401795\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.492801\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.445443\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.291663\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.420599\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.399487\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.325567\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.207892\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.574691\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.530478\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.209221\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.389913\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.217488\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.124585\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.317604\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.135342\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.456684\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.521492\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.187327\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.439416\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.228615\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.269327\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.629545\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.146800\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.363622\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.439043\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.409283\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.236256\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.353759\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.360501\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.227400\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.284694\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.212886\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.178143\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.334955\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.216813\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.196462\n",
      "\n",
      "Test set: Average loss: 0.3430, Accuracy: 8992/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.484495\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.326300\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.509067\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.202080\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.303697\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.493466\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.133516\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.507164\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.182966\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.394326\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.392810\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.305538\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.239934\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.485433\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.213966\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.248701\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.187151\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.119665\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.295323\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.188425\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.451810\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.310360\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.269092\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.557944\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.331263\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.334063\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.206593\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.325982\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.273071\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.258392\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.367933\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.367558\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.484710\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.350641\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.354679\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.366353\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.206834\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.262731\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.189632\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.169826\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.407593\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.289723\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.430760\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.315770\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.387882\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.376427\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.240490\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.333683\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.137520\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.128129\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.526677\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.523304\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.265700\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.206793\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.331628\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.354325\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.307413\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.060587\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.239773\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.131864\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.242031\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.388080\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.660073\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.133031\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.438623\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.211299\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.382874\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.446830\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.131007\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.447697\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.195904\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.517693\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.380193\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.308633\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.433371\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.165778\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.495601\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.153419\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.146854\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.222377\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.252576\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.338936\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.173711\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.536316\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.215550\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.377059\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.161485\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.217731\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.463414\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.319652\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.317219\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.165739\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.490879\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.217901\n",
      "\n",
      "Test set: Average loss: 0.3001, Accuracy: 9146/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.191699\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.390738\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.345276\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.326140\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.200829\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.442733\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.353705\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.221985\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.309700\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.380451\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.304690\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.279000\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.348653\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.205838\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.148670\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.180568\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.608156\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.191378\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.327493\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.285815\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.226400\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.305664\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.237206\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.186642\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.140607\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.190326\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.190891\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.313105\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.401137\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.220997\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.311064\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.413876\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.324181\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.235217\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.267242\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.316915\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.376161\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.228340\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.238283\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.355005\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.269600\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.380310\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.303546\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.452072\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.341613\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.326444\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.365154\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.190241\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.633301\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.216673\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.239508\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.238729\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.287312\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.261449\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.324428\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.372818\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.275263\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.148280\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.318902\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.252912\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.253965\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.267268\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.384862\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.153803\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.289632\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.196565\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.365657\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.306531\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.143417\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.365873\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.241073\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.215577\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.220224\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.437647\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.202016\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.223776\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.444627\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.375245\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.236948\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.379532\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.293679\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.251497\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.358675\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.181513\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.139533\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.157873\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.243550\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.377035\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.270810\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.209005\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.197384\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.178445\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.212801\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.251477\n",
      "\n",
      "Test set: Average loss: 0.2781, Accuracy: 9208/10000 (92%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.113722\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.258973\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.243336\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.509702\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.410939\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.443460\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.338410\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.250451\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.157748\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.252191\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.456892\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.321404\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.234062\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.268939\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.377876\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.356469\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.230790\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.309013\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.149676\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.247951\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.217197\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.211186\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.267139\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.239732\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.290098\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.306173\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.335081\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.568707\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.494060\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.292119\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.341579\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.265540\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.243165\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.195723\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.301188\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.304096\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.573617\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.291769\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.327795\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.414901\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.134592\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.418650\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.208283\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.290098\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.337346\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.205370\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.191182\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.238134\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.205933\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.339846\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.574638\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.256586\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.279792\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.312401\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.389338\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.346379\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.193257\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.235653\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.299108\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.473096\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.261116\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.218797\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.480310\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.406059\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.578095\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.187556\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.180494\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.203419\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.489512\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.342220\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.280564\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.157737\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.499923\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.133290\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.534599\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.229897\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.254506\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.344850\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.203518\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.276974\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.821007\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.372552\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.064353\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.392858\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.392548\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.112362\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.328488\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.134745\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.202093\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.164286\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.260270\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.270609\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.472760\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.201510\n",
      "\n",
      "Test set: Average loss: 0.2757, Accuracy: 9218/10000 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1 + 5):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}