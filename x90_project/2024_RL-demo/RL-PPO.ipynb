{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-02T12:18:46.158882Z",
     "start_time": "2024-07-02T12:18:44.928479Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "一文读懂强化学习：RL全面解析与Pytorch实战\n",
    "https://blog.csdn.net/magicyangjay111/article/details/132645347\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:19:23.585628Z",
     "start_time": "2024-07-02T12:19:23.537041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义策略网络\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(state_dim, 128)\n",
    "        self.policy_head = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        return torch.softmax(self.policy_head(x), dim=-1)\n",
    "\n",
    "\n",
    "# 初始化\n",
    "state_dim = 4  # 状态维度\n",
    "action_dim = 2  # 动作维度\n",
    "epsilon = 0.2\n",
    "count = 20\n",
    "\n",
    "# mock random\n",
    "# states = torch.rand(count, state_dim)\n",
    "# actions = torch.randint(0, action_dim, (count,))\n",
    "# rewards = torch.rand(count)\n",
    "\n",
    "# mock sin\n",
    "data_a = torch.randint(-5, 5, (count, 4)) + torch.rand(count, 4)\n",
    "data_b = torch.sin(data_a)\n",
    "data_c = torch.where(data_b[:, 0] >= 0, 0, 1)\n",
    "states = data_b\n",
    "actions = data_c\n",
    "rewards = torch.rand(count)\n",
    "\n",
    "print(\"states:\", states)\n",
    "print(\"actions:\", actions)\n",
    "print(\"rewards:\", rewards)\n",
    "\n",
    "# net\n",
    "policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "# 计算旧策略的动作概率\n",
    "with torch.no_grad():\n",
    "    old_probs = policy_net(states).gather(1, actions.unsqueeze(-1)).squeeze()\n",
    "\n",
    "# PPO更新 Typically we run multiple epochs\n",
    "for i in range(50):\n",
    "    action_probs = policy_net(states).gather(1, actions.unsqueeze(-1)).squeeze()\n",
    "    ratio = action_probs / old_probs\n",
    "\n",
    "    # loss A\n",
    "    # surr1 = ratio * rewards\n",
    "    # surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * rewards\n",
    "    # loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "    # loss B\n",
    "    loss = -action_probs.mean()\n",
    "\n",
    "    print(f\"Loss: {loss}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"PPO Update Done!\")"
   ],
   "id": "d68a912fa881b769",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: tensor([[ 0.8431,  0.6183, -0.6976,  0.9636],\n",
      "        [-0.9992, -0.9788,  0.6931, -0.2512],\n",
      "        [ 0.8475, -0.2918,  0.8291,  0.1092],\n",
      "        [ 0.5724,  0.0162,  0.8681,  0.1696],\n",
      "        [-0.5243,  0.5001,  0.6195,  0.6132],\n",
      "        [-0.4970,  0.5142,  0.1246,  0.9916],\n",
      "        [-0.3402,  0.5892,  0.9689,  0.2393],\n",
      "        [ 0.0043, -0.7968, -0.4943, -0.9505],\n",
      "        [-0.8316, -0.2831,  0.9955, -0.3262],\n",
      "        [ 0.3890,  0.1835, -0.9353, -0.2703],\n",
      "        [-0.5330,  0.9403, -0.9971,  0.4100],\n",
      "        [ 0.5093, -0.9723, -0.5617, -0.4294],\n",
      "        [ 0.8732, -0.4998, -0.4779,  0.8237],\n",
      "        [-0.1981, -0.9468,  0.2812,  0.2855],\n",
      "        [ 0.5168, -0.9835,  0.9296, -0.2432],\n",
      "        [ 0.2869,  0.2392, -0.6645,  0.0677],\n",
      "        [ 0.9342, -0.9985, -0.9611,  0.8807],\n",
      "        [ 0.2883, -0.2654, -0.9639,  0.9757],\n",
      "        [ 0.9791,  0.3236,  0.0508, -0.9896],\n",
      "        [ 0.9720, -0.5688, -0.7570,  0.9588]])\n",
      "actions: tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "rewards: tensor([0.6615, 0.6629, 0.2327, 0.5177, 0.0192, 0.0752, 0.8310, 0.5996, 0.7715,\n",
      "        0.1745, 0.1195, 0.5465, 0.9004, 0.0783, 0.5449, 0.6739, 0.6859, 0.9140,\n",
      "        0.2166, 0.8341])\n",
      "Loss: -0.482388973236084\n",
      "Loss: -0.49024122953414917\n",
      "Loss: -0.4980959892272949\n",
      "Loss: -0.5059423446655273\n",
      "Loss: -0.513769268989563\n",
      "Loss: -0.52157062292099\n",
      "Loss: -0.5293343663215637\n",
      "Loss: -0.5370510220527649\n",
      "Loss: -0.5447160005569458\n",
      "Loss: -0.5523151159286499\n",
      "Loss: -0.5598441362380981\n",
      "Loss: -0.5672928094863892\n",
      "Loss: -0.5746554136276245\n",
      "Loss: -0.5819256901741028\n",
      "Loss: -0.5890985131263733\n",
      "Loss: -0.5961695313453674\n",
      "Loss: -0.6031320095062256\n",
      "Loss: -0.6099821329116821\n",
      "Loss: -0.6167182922363281\n",
      "Loss: -0.6233357787132263\n",
      "Loss: -0.6298359632492065\n",
      "Loss: -0.6362214088439941\n",
      "Loss: -0.6424949169158936\n",
      "Loss: -0.6486584544181824\n",
      "Loss: -0.6547127962112427\n",
      "Loss: -0.6606646180152893\n",
      "Loss: -0.6665163040161133\n",
      "Loss: -0.6722708940505981\n",
      "Loss: -0.6779335737228394\n",
      "Loss: -0.6835148334503174\n",
      "Loss: -0.6890174746513367\n",
      "Loss: -0.6944429278373718\n",
      "Loss: -0.6997938752174377\n",
      "Loss: -0.7050737142562866\n",
      "Loss: -0.7102839350700378\n",
      "Loss: -0.7154240608215332\n",
      "Loss: -0.7204952836036682\n",
      "Loss: -0.7254987359046936\n",
      "Loss: -0.7304367423057556\n",
      "Loss: -0.7353115081787109\n",
      "Loss: -0.7401205897331238\n",
      "Loss: -0.7448646426200867\n",
      "Loss: -0.7495414018630981\n",
      "Loss: -0.7541480660438538\n",
      "Loss: -0.7586895227432251\n",
      "Loss: -0.7631696462631226\n",
      "Loss: -0.7675831317901611\n",
      "Loss: -0.7719249129295349\n",
      "Loss: -0.7762002348899841\n",
      "Loss: -0.7804052233695984\n",
      "PPO Update Done!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:19:25.894981Z",
     "start_time": "2024-07-02T12:19:25.889545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    new_probs = policy_net(states)  # .gather(1, actions.unsqueeze(-1)).squeeze()\n",
    "    print(torch.cat((data_b[:, 0].unsqueeze(-1), actions.unsqueeze(-1), new_probs), dim=1))\n",
    "    print(\"new_probs:\", new_probs.gather(1, actions.unsqueeze(-1)).squeeze())"
   ],
   "id": "b800758cdbf325a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8431,  0.0000,  0.8339,  0.1661],\n",
      "        [-0.9992,  1.0000,  0.2530,  0.7470],\n",
      "        [ 0.8475,  0.0000,  0.7661,  0.2339],\n",
      "        [ 0.5724,  0.0000,  0.6293,  0.3707],\n",
      "        [-0.5243,  1.0000,  0.2059,  0.7941],\n",
      "        [-0.4970,  1.0000,  0.2671,  0.7329],\n",
      "        [-0.3402,  1.0000,  0.2074,  0.7926],\n",
      "        [ 0.0043,  0.0000,  0.8600,  0.1400],\n",
      "        [-0.8316,  1.0000,  0.1684,  0.8316],\n",
      "        [ 0.3890,  0.0000,  0.8494,  0.1506],\n",
      "        [-0.5330,  1.0000,  0.4939,  0.5061],\n",
      "        [ 0.5093,  0.0000,  0.9259,  0.0741],\n",
      "        [ 0.8732,  0.0000,  0.9183,  0.0817],\n",
      "        [-0.1981,  1.0000,  0.6656,  0.3344],\n",
      "        [ 0.5168,  0.0000,  0.7695,  0.2305],\n",
      "        [ 0.2869,  0.0000,  0.7787,  0.2213],\n",
      "        [ 0.9342,  0.0000,  0.9670,  0.0330],\n",
      "        [ 0.2883,  0.0000,  0.8683,  0.1317],\n",
      "        [ 0.9791,  0.0000,  0.8404,  0.1596],\n",
      "        [ 0.9720,  0.0000,  0.9454,  0.0546]])\n",
      "new_probs: tensor([0.8339, 0.7470, 0.7661, 0.6293, 0.7941, 0.7329, 0.7926, 0.8600, 0.8316,\n",
      "        0.8494, 0.5061, 0.9259, 0.9183, 0.3344, 0.7695, 0.7787, 0.9670, 0.8683,\n",
      "        0.8404, 0.9454])\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
