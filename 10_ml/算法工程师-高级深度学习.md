# 算法工程师-高级深度学习

https://learn.kaikeba.com/catalog/216048

- pytorch doc https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html
- pytorch基础入门教程/一小时学会pytorch https://blog.csdn.net/weixin_41070748/article/details/89890330

## 第二章 神经网络深⼊

- 第1节 从优化问题讲起 I
- 第2节 【实战】拟合问题
    - 代码 demo2-2.ipynb
    - model = nn.Linear(in_features=1, out_features=1) # 单参数
    - loss_fn = nn.MSELoss()
    - optimizer = torch.optim.SGD(model.parameters(), lr=.01)
- 第3节 从优化问题讲起 II
    - gradient descent
    - momentum 动量：相当于 lr 加个 a加速度。为了解决 N 波峰，无法越过峰问题
    - AdaGrad 没太懂，但是变慢很多，lr / 区域面积 导致？梯度与lr反比，大的反而小
    - RMSProp 针对AdaGrad太慢，添加一个（区域面积）滑动窗口
    - Adam 更进一步
- 第4节 【实战】优化方法比较
    - 可视化工具 https://github.com/lilipads/gradient_descent_viz
- 第5节 深度神经网络
    - 激活 Function
        - sigmoid `范围：0 ~ 1`
        - tanh(x) = 2 * sigmoid(2 * x) - 1 `范围：-1 ~ 1`
        - ReLU `范围：>= 0`
        - leaky relu / p-relu 针对 左半区（死区）优化
    - 深度 = 学习力 广度 = 记忆力
- 第6节 【实战】使用神经网络建模MNIST数据
    - 代码 demo2-6.ipynb
